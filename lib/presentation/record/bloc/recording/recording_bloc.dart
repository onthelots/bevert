import 'dart:async';
import 'dart:typed_data';
import 'package:bevert/presentation/record/bloc/recording/recording_event.dart';
import 'package:bevert/presentation/record/bloc/recording/recording_state.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter_bloc/flutter_bloc.dart';
import 'package:permission_handler/permission_handler.dart';
import 'package:vad/vad.dart';

class RecordingBloc extends Bloc<RecordingEvent, RecordingState> {

  /// ë…¹ìŒ ì§„í–‰ ì‹œê°„ì„ ì¶”ì í•˜ëŠ” íƒ€ì´ë¨¸
  Timer? _timer;

  /// ìŒì„± ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²­í¬(chunk)ë¡œ ë§Œë“¤ê¸° ìœ„í•œ íƒ€ì´ë¨¸
  Timer? _chunkTimer;

  /// VADê°€ ê°ì§€í•œ ìŒì„± í”„ë ˆì„(16-bit PCM)ì„ ì„ì‹œë¡œ ì €ì¥í•˜ëŠ” ë²„í¼
  final List<int> _chunkBuffer = [];

  /// VAD íŒ¨í‚¤ì§€ í•¸ë“¤ëŸ¬
  VadHandlerBase? _vadHandler;

  /// ìŒì„± ê°ì§€ ë¯¼ê°ë„ (0.0 ~ 1.0). ë‚®ì„ìˆ˜ë¡ ë¯¼ê°í•¨.
  final double _speechThreshold = 0.3;

  /// ìŒì„± ë°ì´í„°ë¥¼ ëª‡ ì´ˆ ê°„ê²©ìœ¼ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬í• ì§€ ê²°ì •
  final Duration _chunkInterval = const Duration(seconds: 8);

  RecordingBloc() : super(RecordingState()) {
    // ê° ì´ë²¤íŠ¸ ë§¤í•‘
    on<UpdateMeetingInfo>(_onUpdateMeetingInfo);
    on<StartRecording>(_onStartRecording);
    on<PauseRecording>(_onPauseRecording);
    on<ResumeRecording>(_onResumeRecording);
    on<StopRecording>(_onStopRecording);
    on<Tick>(_onTick);
    on<ChunkReady>(_onChunkReady);
    on<AudioChunkReceived>(_onAudioChunkReceived);
    on<UpdateAmplitude>((event, emit) {
      emit(state.copyWith(amplitude: event.amplitude));
    });

    on<SpeechStarted>((event, emit) {
      emit(state.copyWith(segments: [...state.segments, "ğŸŸ¢ Speech started"]));
    });

    on<SpeechEnded>((event, emit) {
      emit(state.copyWith(
          segments: [...state.segments, "ğŸ”´ Speech ended (${event.sampleCount} samples)"]));
    });

    on<SpeechError>((event, emit) {
      debugPrint("VAD Error: ${event.message}");
    });
  }

  /// íšŒì˜ ì œëª©, ë§¥ë½ ì •ë³´ ì—…ë°ì´íŠ¸
  void _onUpdateMeetingInfo(UpdateMeetingInfo event, Emitter<RecordingState> emit) {
    emit(state.copyWith(
      title: event.title,
      meetingContext: event.meetingContext,
    ));
  }

  /// VAD ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
  void _setupVadHandler(Emitter<RecordingState> emit) {
    _vadHandler!.onFrameProcessed.listen((frame) {
      final amplitude = frame.isSpeech;
      if (frame.isSpeech > _speechThreshold) {
        _chunkBuffer.addAll(frame.frame.map((e) => (e * 32767).toInt()));
        print("ğŸ—£ï¸ ë°œí™”ì¤‘ : ${amplitude}");
        add(UpdateAmplitude(amplitude));
      }
    });


    _vadHandler!.onSpeechStart.listen((_) {
      add(SpeechStarted());
    });

    _vadHandler!.onSpeechEnd.listen((samples) {
      add(SpeechEnded(samples.length));
    });

    _vadHandler!.onError.listen((msg) {
      add(SpeechError(msg));
    });
  }

  /// ë…¹ìŒ ì‹œì‘ ì´ë²¤íŠ¸ ì²˜ë¦¬
  Future<void> _onStartRecording(StartRecording event, Emitter<RecordingState> emit) async {
    emit(state.copyWith(
      status: RecordingStatus.initializing,
      duration: 0,
      segments: [],
    ));

    // 1. ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
    final hasPermission = await checkAndRequestMicrophonePermission();
    if (!hasPermission) {
      emit(state.copyWith(status: RecordingStatus.idle));
      return;
    }

    // 2. VAD í•¸ë“¤ëŸ¬ ìƒì„± ë° ë¦¬ìŠ¤ë„ˆ ì—°ê²°
    _vadHandler = VadHandler.create(isDebug: false);
    _setupVadHandler(emit);

    // 3. VAD ë§ˆì´í¬ ì…ë ¥ ì‹œì‘
    await _vadHandler!.startListening();

    // 4. íƒ€ì´ë¨¸ ì‹œì‘
    _chunkTimer = Timer.periodic(_chunkInterval, (_) => _sendChunk());
    _startTimer();

    emit(state.copyWith(status: RecordingStatus.recording));
  }

  /// ë…¹ìŒ ì¼ì‹œì •ì§€ ì´ë²¤íŠ¸ ì²˜ë¦¬
  Future<void> _onPauseRecording(PauseRecording event, Emitter<RecordingState> emit) async {
    _chunkTimer?.cancel();
    await _vadHandler!.pauseListening();
    _stopTimer();

    emit(state.copyWith(status: RecordingStatus.paused));
  }

  /// ë…¹ìŒ ì¬ê°œ ì´ë²¤íŠ¸ ì²˜ë¦¬
  Future<void> _onResumeRecording(ResumeRecording event, Emitter<RecordingState> emit) async {
    await _vadHandler!.startListening();
    _chunkTimer = Timer.periodic(_chunkInterval, (_) => _sendChunk());
    _startTimer();
    emit(state.copyWith(status: RecordingStatus.recording));
  }

  /// ë…¹ìŒ ì¢…ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
  Future<void> _onStopRecording(StopRecording event, Emitter<RecordingState> emit) async {
    _chunkTimer?.cancel();
    _stopTimer();

    if (_chunkBuffer.isNotEmpty) _sendChunk();
    await _vadHandler?.stopListening();
    await _vadHandler?.dispose();
    _vadHandler = null;

    emit(state.copyWith(status: RecordingStatus.stopped));
  }

  /// ì²­í¬ ì „ì†¡
  void _sendChunk() {
    if (_chunkBuffer.isEmpty) return;

    final audioData = _convertPcmToWav(_chunkBuffer);
    _chunkBuffer.clear();

    // Whisper ê°™ì€ STT APIì— ì „ë‹¬í•  ìˆ˜ ìˆìŒ
    add(AudioChunkReceived(audioData));
  }

  /// ChunkReady ì´ë²¤íŠ¸ ì²˜ë¦¬ (ë‹¨ìˆœíˆ í¬ê¸°ë§Œ ë³´ê³  ë¡œê·¸ ê¸°ë¡)
  void _onChunkReady(ChunkReady event, Emitter<RecordingState> emit) {
    debugPrint("ğŸ“¦ ChunkReady ì´ë²¤íŠ¸ ë°œìƒ, size: ${event.size}");
  }

  /// AudioChunkReceived ì´ë²¤íŠ¸ ì²˜ë¦¬ (STT ì—°ë™ ìë¦¬)
  void _onAudioChunkReceived(AudioChunkReceived event, Emitter<RecordingState> emit) {
    debugPrint("ğŸ§ AudioChunkReceived, bytes: ${event.chunk.length}");

    // ì—¬ê¸°ì„œ ì‹¤ì œ STT API í˜¸ì¶œ í›„ transcript ê²°ê³¼ë¥¼ ë°›ì•„ì„œ
    // emit(state.copyWith(segments: [...state.segments, transcript]));
  }

  /// 1ì´ˆë§ˆë‹¤ ë…¹ìŒ ì‹œê°„ì„ ì¦ê°€
  void _onTick(Tick event, Emitter<RecordingState> emit) {
    if (state.status == RecordingStatus.recording) {
      emit(state.copyWith(duration: state.duration + 1));
    }
  }

  //==================================================================
  // Private Helpers
  //==================================================================

  void _startTimer() {
    _timer?.cancel();
    _timer = Timer.periodic(const Duration(seconds: 1), (_) => add(Tick()));
  }

  void _stopTimer() {
    _timer?.cancel();
  }

  Future<bool> checkAndRequestMicrophonePermission() async {
    PermissionStatus status = await Permission.microphone.status;
    if (status.isDenied) status = await Permission.microphone.request();
    if (status.isPermanentlyDenied) {
      await openAppSettings();
      return false;
    }
    return status.isGranted;
  }

  Uint8List _convertPcmToWav(List<int> pcmData) {
    final sampleRate = 16000;
    final numChannels = 1;
    final bitsPerSample = 16;

    final byteData = ByteData(pcmData.length * 2);
    for (int i = 0; i < pcmData.length; i++) {
      byteData.setInt16(i * 2, pcmData[i], Endian.little);
    }
    final pcmBytes = byteData.buffer.asUint8List();

    final header = ByteData(44);
    header.setUint32(0, 0x46464952, Endian.big); // "RIFF"
    header.setUint32(4, 36 + pcmBytes.length, Endian.little);
    header.setUint32(8, 0x45564157, Endian.big); // "WAVE"
    header.setUint32(12, 0x20746d66, Endian.big); // "fmt "
    header.setUint32(16, 16, Endian.little);
    header.setUint16(20, 1, Endian.little);
    header.setUint16(22, numChannels, Endian.little);
    header.setUint32(24, sampleRate, Endian.little);
    header.setUint32(28, sampleRate * numChannels * (bitsPerSample ~/ 8), Endian.little);
    header.setUint16(32, numChannels * (bitsPerSample ~/ 8), Endian.little);
    header.setUint16(34, bitsPerSample, Endian.little);
    header.setUint32(36, 0x61746164, Endian.big); // "data"
    header.setUint32(40, pcmBytes.length, Endian.little);

    return Uint8List.fromList(header.buffer.asUint8List() + pcmBytes);
  }

  @override
  Future<void> close() {
    _timer?.cancel();
    _chunkTimer?.cancel();
    _vadHandler?.dispose();
    return super.close();
  }
}
